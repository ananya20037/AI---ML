{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated θ using Adam: 0.50099999998\n",
      "EMA for g1: -0.5, EMA for g2: 0.2\n",
      "Updated θ after second Adam step (t=2): 0.50199999996\n"
     ]
    }
   ],
   "source": [
    "# Optimizers\n",
    "\n",
    "# Task 1: Adam Optimizer Overview\n",
    "# Given parameter ( \\theta = 0.5 ), use Adam to adaptively update this parameter. Assume ( m_t = 0, v_t = 0, \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8} ).\n",
    "\n",
    "# Task 2: Exponential Moving Averages (EMA)\n",
    "# Calculate EMA over gradients with (\\beta_1 = 0.9) for (g_1 = -0.5) and (g_2 = 0.2).\n",
    "    \n",
    "# Task 3: Adam Update Rule\n",
    "# Perform an Adam update for ( t=2 ), where (g_1 = -0.5), (g_2 = 0.2), with given ( \\beta_1 ) and ( \\beta_2 ).\n",
    "import numpy as np\n",
    "\n",
    "# Title: Optimizers\n",
    "\n",
    "# Task 1: Adam Optimizer Overview\n",
    "# Given parameter θ = 0.5, use Adam to adaptively update this parameter.\n",
    "# Assume: m_t = 0, v_t = 0, β_1 = 0.9, β_2 = 0.999, ε = 10^-8\n",
    "\n",
    "def adam_optimizer(theta, m_t, v_t, g_t, beta_1=0.9, beta_2=0.999, epsilon=1e-8, t=1, learning_rate=0.001):\n",
    "    # Update biased first moment estimate\n",
    "    m_t = beta_1 * m_t + (1 - beta_1) * g_t\n",
    "    \n",
    "    # Update biased second moment estimate\n",
    "    v_t = beta_2 * v_t + (1 - beta_2) * (g_t ** 2)\n",
    "    \n",
    "    # Correct bias in first moment estimate\n",
    "    m_hat = m_t / (1 - beta_1 ** t)\n",
    "    \n",
    "    # Correct bias in second moment estimate\n",
    "    v_hat = v_t / (1 - beta_2 ** t)\n",
    "    \n",
    "    # Update parameter θ using Adam formula\n",
    "    theta_new = theta - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    \n",
    "    return theta_new, m_t, v_t\n",
    "\n",
    "# Initial values\n",
    "theta_initial = 0.5\n",
    "m_t_initial = 0\n",
    "v_t_initial = 0\n",
    "g_t = -0.5  # Gradient at time t\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Perform one step of Adam update for t=1\n",
    "theta_updated, m_t_updated, v_t_updated = adam_optimizer(theta_initial, m_t_initial, v_t_initial, g_t, t=1, learning_rate=learning_rate)\n",
    "print(f\"Updated θ using Adam: {theta_updated}\")\n",
    "\n",
    "# Task 2: Exponential Moving Averages (EMA)\n",
    "# Calculate EMA over gradients with β_1 = 0.9 for g_1 = -0.5 and g_2 = 0.2\n",
    "\n",
    "def exponential_moving_average(g1, g2, beta_1=0.9):\n",
    "    # EMA for two gradients (g_1 and g_2)\n",
    "    ema_1 = beta_1 * g1 + (1 - beta_1) * g1  # EMA for first gradient\n",
    "    ema_2 = beta_1 * g2 + (1 - beta_1) * g2  # EMA for second gradient\n",
    "    \n",
    "    return ema_1, ema_2\n",
    "\n",
    "g_1 = -0.5\n",
    "g_2 = 0.2\n",
    "ema_1, ema_2 = exponential_moving_average(g_1, g_2, beta_1=0.9)\n",
    "print(f\"EMA for g1: {ema_1}, EMA for g2: {ema_2}\")\n",
    "\n",
    "# Task 3: Adam Update Rule\n",
    "# Perform an Adam update for t=2, where g_1 = -0.5, g_2 = 0.2 with given β_1 and β_2.\n",
    "\n",
    "# Perform one more step of Adam update for t=2\n",
    "theta_updated_t2, m_t_updated_t2, v_t_updated_t2 = adam_optimizer(theta_updated, m_t_updated, v_t_updated, g_t, t=2, learning_rate=learning_rate)\n",
    "print(f\"Updated θ after second Adam step (t=2): {theta_updated_t2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
